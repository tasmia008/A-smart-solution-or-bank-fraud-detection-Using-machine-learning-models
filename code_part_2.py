# -*- coding: utf-8 -*-
"""topu_part_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n1azcdUyLVKDcxQGSbVMZrimqOzqY87R
"""

# Commented out IPython magic to ensure Python compatibility.
# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Set Matplotlib to display plots inline in the notebook
# %matplotlib inline

# Increase the maximum number of columns displayed in Pandas to 200
pd.set_option('display.max_columns', 200)

# Set the default style of Matplotlib plots to "ggplot"
plt.style.use('ggplot')

# Read the CSV file into a Pandas DataFrame
df = pd.read_csv("/content/Base.csv")

# Create a deep copy of the DataFrame
new_df = df.copy()

# Get the number of rows and columns in the DataFrame
df_shape = df.shape
print("Number of rows:", df_shape[0])
print("Number of columns:", df_shape[1])

# Display the first 5 rows of the DataFrame
df.head()

"""# 1. Exploratory Data Analysis of Bank Account Applications"""

# Display summary information about the DataFrame
df.info()

# Get the number of unique values in each column of the DataFrame
df.nunique()

# Get a summary of statistical information for each numerical column in the DataFrame
df.describe().transpose()

# Get a summary of statistical information for each non-numerical column in the DataFrame
df.describe(include=["object", "bool"]).transpose()

# Create a new DataFrame showing the count of unique values in the 'fraud_bool' column
fraud_vals = pd.DataFrame(df['fraud_bool'].value_counts())
print(fraud_vals)

# Reset the index of the DataFrame and rename the columns
fraud_vals.reset_index(inplace=True)
fraud_vals.rename(columns={'index': 'fraud_bool', 'fraud_bool': 'count'}, inplace=True)
print(fraud_vals)

# Define custom color palette
my_palette = sns.color_palette("husl", 2)
sns.set_style("whitegrid")

# Count missing values for each column
missing_values_per_column = df.isna().sum()

print("Missing values per column:")
print(missing_values_per_column)

# Create a list of numeric features in the DataFrame df
numeric_features = [x for x in df.columns if df[x].nunique() >= 10]

print(numeric_features)

# Create a grid of subplots
fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(15, 15))

# Add a title to the figure
fig.suptitle('Distribution of Numeric Features by Fraud Status')

# Loop through the numeric features and plot a kernel density plot for each feature
for i, feature in enumerate(numeric_features):
    ax = axes[i // 3][i % 3]
    sns.kdeplot(data=df[df['fraud_bool'] == 0][feature], fill=True, ax=ax, label='Not Fraud')
    sns.kdeplot(data=df[df['fraud_bool'] == 1][feature], fill=True, ax=ax, label='Fraud')
    ax.set_xlabel(feature)
    ax.legend()

# Adjust the layout and display the plot
plt.tight_layout()
plt.show()

# Create a grid of subplots
fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(15, 15))

# Add a title to the figure
fig.suptitle('Box Plot of Numeric Features by Fraud Status')

# Loop through the numeric features and plot a box plot for each feature
for i, feature in enumerate(numeric_features):
    ax = axes[i // 3][i % 3]
    sns.boxplot(data=df, x='fraud_bool', y=feature, ax=ax, palette=my_palette, boxprops=dict(alpha=.6))
    ax.set_xlabel('')
    ax.set_ylabel(feature)
    ax.set_xticklabels(['Not Fraud', 'Fraud'])

# Adjust the layout and display the plot
plt.tight_layout()
plt.show()

# Create a list of categorical features in the DataFrame new_df whose data type is `object`
categorical_features = [x for x in new_df.columns if new_df[x].dtypes == "O"]

print(categorical_features)
print(df.shape[1])

# Convert categorical variables into dummy variables using one-hot encoding
new_df = pd.DataFrame(pd.get_dummies(new_df, prefix=categorical_features))
print(new_df.shape[1])

# Display the first 5 rows of the DataFrame
new_df.head()

# Separate the feature matrix and target variable
X = new_df.drop(['fraud_bool'], axis=1)
y = new_df['fraud_bool']

# Import the necessary libraries for data preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import MinMaxScaler

# Scale the numeric features in the training and testing sets using MinMaxScaler
numeric_transformer = MinMaxScaler()

# Define the ColumnTransformer object with the numeric transformer and the list of numeric features
preprocessor = ColumnTransformer([('scaled', numeric_transformer, numeric_features)], remainder='passthrough')

# Fit the preprocessor on the training set and transform both the training and testing sets
X_scaled = preprocessor.fit_transform(X)
# X_test_scaled = preprocessor.transform(X_test)

# Get the names of the features after pre-processing
feature_names = preprocessor.get_feature_names_out()

# Remove 'remainder__' from the feature names
feature_names = [feature.replace('remainder__', '') for feature in feature_names]

# Print the pre-processed feature names
print(feature_names)

# Convert the scaled training and testing sets to pandas DataFrames
X_scaled = pd.DataFrame(X_scaled, columns=feature_names)

# Import the necessary libraries for feature selection
from sklearn.feature_selection import VarianceThreshold, SelectKBest, SelectFromModel, chi2, mutual_info_classif
from sklearn.linear_model import Lasso, Ridge
from sklearn.ensemble import ExtraTreesClassifier

# Import the necessary libraries for undersampling imbalanced datasets
from imblearn.under_sampling import NearMiss
from collections import Counter

pip install --upgrade numpy scipy

# Create a list of categorical features in the DataFrame X_scaled
categorical_features = [feature for feature in X_scaled.columns if X_scaled[feature].nunique() >= 2 and X_scaled[feature].nunique() < 10]

print(categorical_features)

# Create a list of numerical features in the DataFrame X_scaled
numeric_features = [feature for feature in X_scaled.columns if X_scaled[feature].nunique() >= 10]

print(numeric_features)

# Instantiate a VarianceThreshold selector and fit it to  training set
selector = VarianceThreshold()
selector.fit(X_scaled)

# Get the names of the constant features
constant_features = [feature for feature in X_scaled.columns
                     if feature not in X_scaled.columns[selector.get_support()]]

# Print the names of the constant features
print(constant_features)

# Dropping constant features
X_scaled.drop(['device_fraud_count'], axis=1, inplace=True)

# Compute correlation matrix
X_train_corr = X_scaled[numeric_features].corr()

# Plot correlation matrix with annotated values
fig, ax = plt.subplots(figsize=(18, 10))
sns.heatmap(X_train_corr[(X_train_corr >= 0.4) | (X_train_corr <= -0.4)], annot=True, fmt=".2f", cmap='coolwarm', linewidths=.5, square=True, ax=ax, annot_kws={"fontsize": 8})
plt.title('Correlation Heatmap')
plt.show()

"""### 3.3 Chi-Squared Test for Categorical Features"""

# Check the distribution of values in a column
X_scaled.device_distinct_emails_8w.value_counts()

# Identify the rows to drop based on a condition on a specific feature
rows_to_drop = np.where(X_scaled['device_distinct_emails_8w'] < 0)[0]
rows_to_drop

# Drop the corresponding rows in X_train and y_train
X_train_cat = X_scaled[categorical_features].copy()
X_train_cat.drop(rows_to_drop, axis=0, inplace=True)


y_train_cat = y.copy()
y_train_cat.drop(rows_to_drop, axis=0, inplace=True)

# Impute missing values in X_train_cat using the most frequent value
for col in X_train_cat.columns:
  most_frequent_value = X_train_cat[col].mode()[0]
  X_train_cat[col].fillna(most_frequent_value, inplace=True)

# Use chi-squared test to evaluate the relationship between categorical features and the target variable
chi2_results = chi2(X_train_cat, y_train_cat)

# Create a pandas DataFrame to store the chi-squared test results
chi2_results_df = pd.DataFrame(data={'feature': X_train_cat.columns, 'chi2': chi2_results[0], 'p_value': chi2_results[1]})

# Set up plot
plt.figure(figsize=(16, 8))

# Create bar plot
sns.barplot(data=chi2_results_df.sort_values(by='chi2', ascending=False), x="feature", y="chi2", palette=my_palette, alpha=.6)

# Customize labels and legend
plt.xlabel("Features", fontsize=12)
plt.ylabel("chi2", fontsize=12)
plt.xticks(rotation=90, fontsize=8)
plt.yticks(fontsize=5)
plt.title("Chi-Squared Value by Categorical Feature", fontsize=14)

# Adjust the spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

# Use SelectKBest with chi-squared test to select the top 5 categorical features that are most strongly associated with the target variable
best_chi2_cols = SelectKBest(chi2, k=15)
best_chi2_cols.fit(X_train_cat, y_train_cat)

# Get the names of the top 5 features
best_chi2_features = [X_train_cat.columns[best_chi2_cols.get_support()]]

# Print the names of the top 5 features
print(best_chi2_features)

"""### 3.4 Mutual Information Test for Numeric Features"""

# Create a new DataFrame with only the numeric features from the preprocessed training data
X_train_num = X_scaled[numeric_features].copy()

# Create a copy of the target variable
y_train_num = y.copy()

# Create a new DataFrame with only the numeric features from the preprocessed training data
X_train_num = X_scaled[numeric_features].copy()

# Create a copy of the target variable
y_train_num = y.copy()

# Import necessary libraries
import pandas as pd
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import LinearRegression

# Initialize the PMM-like imputer
imputer = IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state=42, tol=0.001)

# Impute missing values in X_train_num using the imputer
X_train_num_imputed = pd.DataFrame(imputer.fit_transform(X_train_num), columns=X_train_num.columns)

# Use mutual information test to evaluate the relationship between numerical features and the target variable
mutual_info_results = mutual_info_classif(X_train_num_imputed, y_train_num)

# Create a pandas DataFrame to store the mutual information test results
mutual_info_results_df = pd.DataFrame(data={'feature': X_train_num.columns, 'mutual_info': mutual_info_results})

# Create bar plot
sns.barplot(data=mutual_info_results_df.sort_values(by='mutual_info', ascending=False), x="feature", y="mutual_info", palette=my_palette, alpha=.6)

# Customize labels and legend
plt.xlabel("Features", fontsize=12)
plt.ylabel("mutual_info", fontsize=12)
plt.xticks(rotation=45, fontsize=8)
plt.yticks(fontsize=8)
plt.title("Mutual Information Value by Numerical Feature", fontsize=14)

# Adjust the spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

# Use SelectKBest with mutual information test to select the top 5 numerical features that are most strongly associated with the target variable
best_mutual_info_cols = SelectKBest(mutual_info_classif, k=15)
best_mutual_info_cols.fit(X_train_num_imputed, y_train_num)

# Get the names of the top 5 features
best_mutual_info_features = [X_train_num_imputed.columns[best_mutual_info_cols.get_support()]]

# Print the names of the top 5 features
print(best_mutual_info_features)

"""### 3.5 Extra Trees Classifier for Feature Selection"""

extra = ExtraTreesClassifier(n_estimators=50, random_state=0)
extra.fit(X_scaled, y)

# Create a SelectFromModel object with the fitted Random Forest model
feature_sel_extra = SelectFromModel(extra, prefit=True)

# Get the names of the selected features by calling .get_support() on the SelectFromModel object
best_extra_features = [X_scaled.columns[(feature_sel_extra.get_support())]]
best_extra_features = list(best_extra_features[0])

# Print the names of the selected features
print(best_extra_features)

extra_importances = pd.DataFrame({'feature': X_scaled.columns, 'importance': extra.feature_importances_, 'model': 'ExtraTreesClassifier'})
extra_importances.head()

# Set up plot
plt.figure(figsize=(16, 8))

# Create grouped bar plot
sns.barplot(data=extra_importances.sort_values(by='importance', ascending=False), x="feature", y="importance", hue="model", palette=my_palette, alpha=.6)

# Customize labels and legend
plt.xlabel("Features", fontsize=12)
plt.ylabel("Importance Value", fontsize=12)
plt.xticks(rotation=90, fontsize=8)
plt.yticks(fontsize=8)
plt.title("Feature Importances by Model", fontsize=14)
plt.legend(title='Model', fontsize=12)

# Adjust the spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

# Import the necessary libraries for oversampling imbalanced datasets
from imblearn.over_sampling import SMOTENC
from imblearn.pipeline import make_pipeline

# Import necessary modules for hyperparameter tuning
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV

from sklearn.model_selection import train_test_split
# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)

pip install CatBoost

from imblearn.combine import SMOTETomek
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from sklearn.experimental import enable_iterative_imputer  # Enable IterativeImputer
from sklearn.impute import IterativeImputer
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LinearRegression

# Initialize models
models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, eval_metric='logloss', random_state=42),
    'CatBoost': CatBoostClassifier(silent=True, random_state=42),
    'LightGBM': LGBMClassifier(random_state=42, verbose=0),
}

# Set up cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Initialize the PMM-like imputer
imputer = IterativeImputer(estimator=LinearRegression(), max_iter=20, random_state=42, tol=0.001)

# Define the function to evaluate models with sampling methods
def evaluate_models_with_sampling(X, y, sampling_method='SMOTE', sampling_strategy=0.7):
    # Define the sampling strategies
    sampling_methods = {
        'SMOTE': SMOTE(sampling_strategy=sampling_strategy, random_state=42),
        'SMOTETomek': SMOTETomek(sampling_strategy=sampling_strategy, random_state=42),
        'Undersample': RandomUnderSampler(random_state=42),
        'Oversample': RandomOverSampler(random_state=42),
        'ADASYN': ADASYN(sampling_strategy=sampling_strategy, random_state=42)
    }

    # Select the resampling method
    if sampling_method not in sampling_methods:
        raise ValueError(f"Sampling method '{sampling_method}' is not recognized.")

    sampler = sampling_methods[sampling_method]

    # Initialize results dictionary
    cv_results = {name: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': []} for name in models.keys()}

    # Cross-validation loop
    for fold, (train_index, test_index) in enumerate(cv.split(X, y)):
        print(f"Processing fold {fold + 1}/{cv.n_splits}...")

        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # Apply PMM-like imputation
        X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
        X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)

        # Resample the training data
        X_train_res, y_train_res = sampler.fit_resample(X_train_imputed, y_train)

        # Train and evaluate each model
        for name, model in models.items():
            model.fit(X_train_res, y_train_res)
            y_pred = model.predict(X_test_imputed)
            y_prob = model.predict_proba(X_test_imputed)[:, 1] if hasattr(model, "predict_proba") else np.zeros(len(y_pred))

            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            auc = roc_auc_score(y_test, y_prob) if y_prob.any() else 0.0

            # Append results
            cv_results[name]['accuracy'].append(accuracy)
            cv_results[name]['precision'].append(precision)
            cv_results[name]['recall'].append(recall)
            cv_results[name]['f1'].append(f1)
            cv_results[name]['auc'].append(auc)

# Initialize the models
models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, eval_metric='logloss', random_state=42),
    'CatBoost': CatBoostClassifier(silent=True, random_state=42),
    'LightGBM': LGBMClassifier(random_state=42, verbose=0),
}

# Set up 5-fold cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Initialize the PMM-like imputer
imputer = IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state=42, tol=0.001)

# Define a function to evaluate models with different sampling methods
def evaluate_models_with_sampling(X, y, sampling_method='SMOTE', sampling_strategy=0.7):
    # Define the sampling strategy dictionary
    sampling_methods = {
        'SMOTE': SMOTE(sampling_strategy=sampling_strategy, random_state=42),
        'SMOTETomek': SMOTETomek(sampling_strategy=sampling_strategy, random_state=42),
        'Undersample': RandomUnderSampler(random_state=42),
        'Oversample': RandomOverSampler(random_state=42),
        'ADASYN': ADASYN(sampling_strategy=sampling_strategy, random_state=42)
    }

    # Select the resampling method
    if sampling_method not in sampling_methods:
        raise ValueError(f"Sampling method '{sampling_method}' is not recognized.")

    sampler = sampling_methods[sampling_method]

    # Initialize dictionary to store the results for each model
    cv_results = {name: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': []} for name in models.keys()}

    # Perform 5-fold CV with the chosen sampling method
    for train_index, test_index in cv.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # Apply PMM-like imputation on both training and test sets
        X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
        X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)

        # Apply the chosen sampling method to the training data
        X_train_res, y_train_res = sampler.fit_resample(X_train_imputed, y_train)

        # Train each model on the resampled training data and evaluate on the test fold
        for name, model in models.items():
            model.fit(X_train_res, y_train_res)
            y_pred = model.predict(X_test_imputed)
            y_prob = model.predict_proba(X_test_imputed)[:, 1]  # For AUC

            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)
            auc = roc_auc_score(y_test, y_prob)

            # Append the results to the dictionary
            cv_results[name]['accuracy'].append(accuracy)
            cv_results[name]['precision'].append(precision)
            cv_results[name]['recall'].append(recall)
            cv_results[name]['f1'].append(f1)
            cv_results[name]['auc'].append(auc)

    # Display the mean and standard deviation of the metrics for each model
    for name, metrics in cv_results.items():
        print(f"{name} 5-Fold CV Results with {sampling_method}:")
        print(f"Mean Accuracy: {np.mean(metrics['accuracy']):.4f} ± {np.std(metrics['accuracy']):.4f}")
        print(f"Mean Precision: {np.mean(metrics['precision']):.4f} ± {np.std(metrics['precision']):.4f}")
        print(f"Mean Recall: {np.mean(metrics['recall']):.4f} ± {np.std(metrics['recall']):.4f}")
        print(f"Mean F1 Score: {np.mean(metrics['f1']):.4f} ± {np.std(metrics['f1']):.4f}")
        print(f"Mean AUC: {np.mean(metrics['auc']):.4f} ± {np.std(metrics['auc']):.4f}")
        print("="*30)

# Evaluate models with different sampling techniques
evaluate_models_with_sampling(X_train, y_train, sampling_method='ADASYN', sampling_strategy=0.7)
evaluate_models_with_sampling(X_train, y_train, sampling_method='SMOTE', sampling_strategy=0.7)
evaluate_models_with_sampling(X_train, y_train, sampling_method='Undersample')
evaluate_models_with_sampling(X_train, y_train, sampling_method='Oversample')
evaluate_models_with_sampling(X_train, y_train, sampling_method='SMOTETomek', sampling_strategy=0.7)

# Importing the necessary libraries for metrics and evaluation
from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix, auc, roc_curve

"""### 5.1 Classification Report for Model Testing"""

def print_cls_report(y_test, y_pred, title):
    # Calculate the classification report
    default_report = classification_report(y_test, y_pred, target_names=['No Fraud', 'Fraud'])

    # Calculate precision, recall, f1 score and support for each class
    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred)

    # Print the title and the default classification report
    print(title)
    print('*****' * 10)
    print(default_report)

    # Return the recall scores for each class
    return recall

"""### 5.2 Confusion Matrix for Model Testing"""

def plot_con_matrix(ax, y_test, y_pred, title):
    # Define the classes of the classification problem
    classes = ['No Fraud', 'Fraud']

    # Compute the confusion matrix
    con_matrix = confusion_matrix(y_test, y_pred)

    # Compute the values for true negatives, false positives, false negatives, and true positives
    tn, fp, fn, tp = con_matrix.ravel()

    # Compute the false positive rate
    fpr = fp / (fp + tn)

    # Plot the confusion matrix using a heatmap
    ax.imshow(con_matrix, interpolation='nearest', cmap=plt.cm.Blues)

    # Define the tick marks and the labels for the plot
    tick_marks = np.arange(len(classes))
    ax.set_xticks(tick_marks)
    ax.set_xticklabels(classes)
    ax.set_yticks(tick_marks)
    ax.set_yticklabels(classes)

    # Add the count of each cell of the confusion matrix to the plot
    fmt = 'd'
    threshold = con_matrix.max() / 2.
    for i, j in np.ndindex(con_matrix.shape):
        ax.text(j, i, format(con_matrix[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if con_matrix[i, j] > threshold else "black")

    # Add labels to the plot
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')
    ax.set_title(f'{title} with {fpr*100:.2f}% FPR')

"""### 5.3 Cross-Validation Results for Model Testing"""

def print_cv_results(model):
    # Get the parameter and score arrays from the cv_results_ dictionary
    means = model.cv_results_['mean_test_score']
    params = model.cv_results_['params']

    # Combine the arrays using zip()
    combined_results = zip(means, params)

    # Sort the combined array by mean_test_score in descending order
    sorted_results = sorted(combined_results, key=lambda x: x[0], reverse=True)

    # Print the mean test score and the hyperparameters as a formatted string
    for mean, param in sorted_results:
        print("mean_test_score: %f, params: %r" % (mean, param))

"""### 5.4 ROC-AUC for Model Testing"""

def plot_roc_curves(fpr_list, tpr_list, label_list):
    plt.figure(figsize=(8, 8))
    for i in range(len(fpr_list)):
        # Compute the ROC AUC score
        roc_auc_score = auc(fpr_list[i], tpr_list[i])
        # Plot the ROC curve
        plt.plot(fpr_list[i], tpr_list[i], label=f'{label_list[i]} (AUC={roc_auc_score:.2f})')

    # Plot the random classifier curve
    plt.plot([0, 1], [0, 1], 'k--', label='Random')

    # Set the plot labels and title
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()